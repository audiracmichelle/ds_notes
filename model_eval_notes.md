# Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning

## Abstract

This article provides a series of techniques and recommendations regarding:

* model evaluation
* model selection
* algorithm selection

Keywords: hold-out, bootstrap, leave-one out and k-fold cross-validation, bias-variance trade off

## 1 Introduction: Essential Model Evaluation Terms and Techniques

When fitting a model to training data you have to make sure the model generalizes well to unseen data.

### 1.1 Performance Estimation: Generalization Performance vs Model Selection

Why do we care about performance estimates after all? To rank models against each other oftentimes in terms of predictive and computational performance.

PREDICTIVE PERFORMANCE = GENERALIZATION PERFORMANCE

### 1.2 Assumptions and Terminology

Target function -> "The true model"
Hypothesis space -> The universe of models that are to be tested

### 1.3 Re-substitution validation and the Holdout Method

Re-substitution -> Train set = Test set
Holdout method -> One train set and one test set






